---
categories:
- AI/ML
- Notes
date: 2020-10-21 23:02:32 +0900
math: true
tags:
- classification
- measurement
- metrics
- tensorflow
- 딥러닝
- 분류
- 평가
title: '[딥러닝] 분류 모델의 성능평가 방법(Precision, Recall, Accuracy, F1Score, Confusion Matrix)'
uid: 101
---

> [딥러닝] 분류 모델의 성능평가 방법(Precision, Recall, Accuracy, F1Score, Confusion Matrix)
> 

## Confusion Matrix

- 분류모델에서 아래와 같은 형태로 표현된 테이블
    
    ![https://i.imgur.com/cI6fqp3.png](https://i.imgur.com/cI6fqp3.png)
    
- True Positive : 정답은 True이고, 분류 결과도 True인 것 (정답)
- False Positive: 정답은 False이고, 분류 결과는 True인 것 (오답)
- False Negative: 정답은 True이고, 분류 결과는 False인 것 (오답)
- True Negative: 정답은 False이고, 분류 결과도 False인 것 (정답)

## Precision(정밀도)

- 모델이 True라고 한 것 중에 실제 정답이 True인 것 (모델의 관점)
- 위 confusion matrix에서 $Precision=\dfrac{TP}{TP+FP}$
- 0~1의 값을 가지며 높을수록 좋음

## Recall(재현율)

- 실제 정답이 True인 것 중에 모델이 True라고 한 것 (데이터의 관점)
- 위 confusion matrix에서 $Recall=\dfrac{TP}{TP+FN}$
- 0~1의 값을 가지며 높을수록 좋음

## F1 Score

- Precision과 Recall의 조화평균
- $F1\ Score = \dfrac{2}{\dfrac{1}{Precision}+\dfrac{1}{Recall}}$
- 0~1 값을 가지며 높을수록 좋음

## Accuracy(정확도)

- 실제 True를 모델이 True라고 예측한것 + 실제 False를 모델이 False라고 예측한 것의 비율
- 위 confusion matrix에서 $Acc=\dfrac{TP+TN}{TP+FN+FP+TN}$
- 0~1의 값을 가지며 높을수록 좋음

## 소스코드

[여기](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)에서 tensorflow에서 제공하는 다양한 metric 문서를 확인할 수 있고, 각 모델에 맞는 metric을 사용하면 된다. 아래는 이진 분류 모델의 정확도를 계산하는 코드이다.

```python
import tensorflow as tf

y_true = [0,0,1,1]
y_pred = [1,0,1,0]

metric = tf.keras.metrics.BinaryAccuracy()
acc = metric(y_true, y_pred)
print(acc)
# tf.Tensor(0.5, shape=(), dtype=float32)
```