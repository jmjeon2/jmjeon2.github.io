---
categories:
- AI/ML
- Papers
date: 2024-10-25 23:51:24 +0900
tags:
- Detection
- paper
- sam
- segmentation
title: '[논문리뷰] Segment Anything'
uid: 260
---

> [논문리뷰] Segment Anything
> 
- Paper: [https://arxiv.org/abs/2304.02643](https://arxiv.org/abs/2304.02643)
- Code: [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)
- Docs: [https://segment-anything.com/](https://segment-anything.com/)

## 개요

- NLP 분야는 웹에서 쉽게 접근할 수 있는 방대한 텍스트 데이터가 존재하고 쉽게 프롬프팅이 가능하다.
- 따라서 대규모 데이터셋으로 ChatGPT나 Llama와 같은 LLM 모델들이 폭발적인 성장을 이뤘다.
- 하지만 이미지 분야에서는 이러한 규모의 데이터나 프롬프트를 다루기가 쉽지 않기 때문에 Foundation Model 이라고 할 만한 일반적인 모델을 구축하기가 어렵다.
- 따라서 이 논문에서는 위의 문제를 해결하기 위해 3가지의 큰 idea를 제시한다.
    1. Task: LLM처럼 프롬프트가 가능한 segmentation Task
    2. Model: 입력(**다양한 flexible 프롬프트**)과 출력(segmentation mask)이 **real time**으로 동작하도록 하는 Model 구조 설계
    3. Data: 모델과 보조/반자동/자동을 이용한 학습용 data 구축 engine 개발 

## 1. Task

Segmentation Task의 프롬프트는 **이미지 내의 좌표, rough box/mask, 텍스트** 등이 될 수 있다. 이런 `promptable segmentation task`는 아래 이미지와 같이 가방을 선택한건지, 사람을 선택한건지 애매한 경우한 경우에 모두 합리적인 multiple mask가 출력이 되도록 한다.

![스크린샷 2024-10-23 오후 5.11.31.png](https://i.imgur.com/4VEjF6B.jpeg)

### Pretraining

사전학습의 목표는 이 model이 위와같이 애매한 프롬프트가 들어온 경우에 합리적인 output mask가 나오는 것을 목표로 한다.

이는 annotation을 자동으로 만드는 과정에도 활용될 수 있다. 

### Zero-shot transfer

사전학습된 SAM 모델을 활용해서 다양한 Task에 오직 프롬프트만을 이용하여 zero-shot inference를 할 수 있다. 예를 들면 이미지 내에 고양이가 있고, 고양이에 대한 바운딩박스가 프롬프트로 입력되면 box안의 고양이만 segmentation mask가 생성되게 된다.

### Related tasks

기존의 다양한 segmentation task(edge detection, pixelization, object proposal, foreground, semantic, instance, panoptic 등)에 모두 사용될 수 있고, 각각의 모델을 prompt engineering만으로 segmentation이 될 수 있도록 일반화된 모델을 만든다.

즉, 학습하지 않은 데이터나 Task에도 prompt만으로 segmentation이 가능하도록 한다. 

> Panoptic Segmentation?
> 
> Panoptic Segmentation은 Semantic Segmentation과 Instance Segmentation을 결합한 결과이다. 배경을 포함하여 모든 객체와 background를 각각 instance 단위로 구분하여 pixel 단위 결과를 만든다.
> 
> ![image.png](%5BSAM%5D%20Segment%20Anything%2012348a6e55fc80eca7b4e155ad70033c/image.png)
> 
> *출처: [https://viso.ai/deep-learning/panoptic-segmentation/](https://viso.ai/deep-learning/panoptic-segmentation/)*
{: .prompt-tip }


## 2. Model

### SAM 모델 구조

SAM은 아래 그림과 같이 크게 3가지 구조로 나뉜다. 

1. 이미지 인코더, 2. flexible 프롬프트 인코더, 3. Fast Mask 디코더

![스크린샷 2024-10-18 오후 5.51.52.png](https://i.imgur.com/oQCphEs.png)

### Image Encoder

- 이미지 인코더로는 Pretrinaed MAE(고해상도 처리 ViT 기반) 모델을 사용한다.
- 이 모델로부터 이미지 임베딩을 만들어 다음의 프롬프트 인코더와 결합된다.

> MAE 모델이란?
> 
> MAE는 Masked Autoencoder로 아래와 같이 원래의 이미지를 patch 단위로 쪼개어 랜덤하게 masking 한 이후에 전체 이미지를 복원하는 방법으로 학습하는 모델이다.
> 
> SAM은 이 모델 구조에서 Encoder만 가져와서 Image Embedding을 생성하는데 사용한다.
> 
> ![image.png](%5BSAM%5D%20Segment%20Anything%2012348a6e55fc80eca7b4e155ad70033c/image%201.png)
{: .prompt-tip }


### Prompt Encoder

프롬프트 종류에 따라 각각 embedding 형태로 변환한다.

1. Points: 이미지 내의 좌표값에 해당하는 Positional Embedding과 points를 나타내는 Learned Embedding을 더해서 사용
2. Boxes: points와 유사하게 좌상단, 우하단의 좌표에 해당되는 Positional Embedding과 box를 나타내는 Learned Embedding을 더해서 사용
3. Text: Pretrained CLIP의 텍스트 인코더를 사용하여 임베딩 생성
4. Masks: 이미지와 resolution이 동일하기 때문에 convolution 연산 이후에 Image 임베딩과 element-wise로 더해짐 

### Mask decoder

- decoder는 이미지 임베딩, 프롬프트 임베딩, 출력 토큰을 mask로 만들어준다.
- Prompt와 Image 임베딩 간의 self-attention, cross-attention을 사용하여 서로 상호관계를 학습하도록 한다.
- 이후에 만들어진 임베딩을 upsample하며 출력 token을 mask prediction head를 통해 최종 mask를 출력한다.

![스크린샷 2024-10-23 오후 5.38.40.png](https://i.imgur.com/kthIZov.png)

### Resolving ambiguity

- 하나의 mask 출력만 사용하면 프롬프트가 모호한 경우 출력을 어떤 instance로 정할지 알 수 없다.
- 따라서 3개의 output mask를 만들었고, 3개면 충분히 전체, 부분, 세부 부분까지 구분되는 mask가 생성된다.
- 학습할 때에는 모든 출력 mask 중에 최소 loss에 해당되는 mask만 학습하였고, 최종 출력 mask들의 순위를 정하기 위해 confidence score(estimated IoU)를 사용하였다.

### Efficiency

- 전반적으로 효율성이 매우 높도록 설계되었다.
- 이미지 임베딩을 제외하면 프롬프트 인코더와 mask 디코더는 **cpu 로 50ms 이내로 동작**이 가능하여 웹에서도 real-time으로 구동이 가능하다.

### Losses and Training

- 학습 Loss는 focal loss와 dice loss를 linear하게 weight하여 사용하였다.

## 3. Data Engine

인터넷에는 학습에 필요한 충분한 Annotation 데이터가 없기 때문에 SA-1B라는 데이터셋을 만들었다. 이 데이터는 자체적으로 구축한 data engine으로 만들었는데 1. 수동 2. 모델보조(반자동) 3. 완전자동 의 단계로 만들어졌다

### Assisted-manual stage

- 자체적으로 Annotation 할 수 있도록 web 기반 Tool 을 만들어 수작업을 통해 데이터 생성
- SAM 모델이 출력한 mask를 보정할 수 있는 기능이 있음
- 한장에 30초 이내로 만들도록 요청하였고, 주요 객체 위주로 생성하도록 하였음
- 생성된 mask를 통해 모델을 재학습하고 반복할수록 mask annotation 시간이 효율화됨
- 총 120k 이미지로부터 3.4M 의 annotation 데이터 생성

### Semi-automatic stage

- 모델 성능 개선을 위해 마스크의 다양성이 올라가도록 함
- 대부분 SAM 모델이 출력하는 Mask를 보정하여 Annotation 시간 매우 단축
- SAM 모델이 생성하지 못하는 주요 객체가 아닌 것들도 Annotation 수작업 진행
- 총 180k 이미지로부터 5.9M의 annotation 데이터 생성 (누적 10.2M)

### Fully automatic stage

- 32x32 grid 포인트로 프롬프트를 넣어 영상내의 대부분의 객체 mask를 생성하도록 함
- 생성된 마스크들은 whole, part, subpart를 모두 포함하게 되었고, confidence가 높은 valid mask만 남긴 이후에 NMS 알고리즘을 통해 중복 마스크를 제거함
- 총 11M 이미지로부터 1.1B의 고퀄리티 Annotation 마스크를 생성함

## 4. Dataset

**Image**

- 총 11M 규모의 이미지
- 평균 3300x4950 resolution의 데이터를 shortest 부분을 1500 이 되도록 resize
- 타 데이터셋 대비 굉장히 고품질 이미지

**Mask**

- 총 1.1B 규모의 Mask 이미지
- 전체 데이터중 99.1%는 완전 자동으로 만들어짐
- 자동 생성 Mask가 굉장히 고품질로 SA-1B에는 모두 자동생성 mask만 포함됨

아래 그래프를 보면 SA-1B가 타 데이터셋 대비 규모가 월등히 크고, 한 이미지당 Mask의 개수가 많은 비율이 훨씬 높음을 알 수 있다. 또한, 크기가 작은 mask의 개수가 많아 상대적으로 세밀한 segmentation 자료가 있음을 알 수 있다.

![스크린샷 2024-10-25 오후 2.15.08.png](https://i.imgur.com/ejwfBUV.png)

*SA-1B 데이터는 타 데이터와 다르게 mask의 중심 분포가 매우 고르게 되어있다 (다양성)*

![스크린샷 2024-10-23 오후 6.15.14.png](https://i.imgur.com/rwWblgb.png)

## 5. Zero-shot Transfer Experiments

SAM 모델을 이용하여 단지 prompt만을 이용하여 아래 5개의 Task를 잘 수행하는지 각각 테스트를 하였다.

### Zero shot single point Valid Mask Evaluation

- 아래는 23개의 데이터셋에 대해 SAM과 RITM의 결과를 비교한 결과이다.
- 주황색 동그라미 표시는 SAM의 ambiguity때문에 3장 출력되는 mask중 gt와 가장 유사한 것으로 평가한 결과이다. → “oracle”
- oracle 평가 결과 모든 데이터셋에 대해 SAM 모델이 우수한 결과를 보인다.

![스크린샷 2024-10-25 오후 2.44.05.png](https://i.imgur.com/1Id69uf.png)

- 아래 (b)그래프를 통해 SAM의 “oracle” 성능이 가장 우수한것을 확인할 수 있다.
- (c)와 (d)그래프를 보면 point prompt 개수가 적을때에는 SAM이 압도적으로 성능이 좋다. point 개수가 많아지면 task가 쉬워지기 때문에 타 모델과의 성능 gap이 줄어들게 된다.
    - Center Points: 객체의 중심점 클릭
    - Random Points: 객체의 아무 부분이나 클릭

![스크린샷 2024-10-25 오후 2.50.10.png](https://i.imgur.com/aoFiYVx.png)

### Zero shot Edge Detection

- Edge Detection을 하기 위해 추가적으로 학습하지 않고 이미지를 16x16의 grid로 나눠 foreground point로 생성하여 총 768개의 mask를 생성한다. (포인트별 3개 마스트 출력)
- NMS 알고리즘을 통해 중복되는 mask는 삭제하고, edge map은 sobel filter를 통해 만들어낸다.
- edge detection을 따로 학습하지 않아도 아래 이미지처럼 꽤 좋은 퀄리티의 출력이 생성된다.

![스크린샷 2024-10-25 오후 2.56.20.png](https://i.imgur.com/hfBx1RD.png)

![스크린샷 2024-10-25 오후 2.58.59.png](https://i.imgur.com/MfcUCHS.png)

### Zero shot Object Proposal

- LVIS v1 데이터에 대해 학습되지 않은 SAM 모델이 SOTA 대비(ViTDet-H) Average Recall 성능에서 상당히 높게 나온다.
- 특히 medium, large 크기의 객체에서는 거의 유사한 성능을 보인다.

![스크린샷 2024-10-25 오후 3.02.31.png](https://i.imgur.com/JJ8hR4P.png)

### Zero shot Instance Segmentation

- SAM 이전에 Object Detector를 사용하여 검출된 object들을 prompt로 입력하여 해당 객체를 segmentation하는 task이다.
- SOTA모델 ViTDet-H(Fully-Supervised)와 비교시에는 성능이 떨어지지만, 고퀄리티 LVIS 마스크에서는 성능 차이가 줄어든다.
- 하지만 사람들이 직접 수행한 정성평가에서는 ViTDet보다 좀 더 우세하게 평가된다. (SAM은 고퀄 mask로 학습, ViTDet는 상대적으로 퀄리티가 낮은 COCO mask로 학습했기 때문이고 설명됨)

![스크린샷 2024-10-25 오후 3.11.09.png](https://i.imgur.com/gvI6zjl.png)

![스크린샷 2024-10-25 오후 3.11.16.png](https://i.imgur.com/xSG0jW9.png)

### Zero shot Text to Mask

- 다른 Task와 다르게 Image와 Text를 Embedding 차원에서 Align하기 위한 추가 학습을 진행하였고, 실제 테스트 시에는 CLIP Text Encoder 를 통해 text embedding을 추출하였다.
- 아래 이미지를 보면 텍스트로 정확한 mask생성에 실패한경우, point 등 추가적인 프롬프트를 넣어주면 개선된다.

![스크린샷 2024-10-25 오후 3.16.27.png](https://i.imgur.com/D3Ci5Jr.jpeg)

Uploaded by [N2C](https://github.com/jmjeon2/Notion2Chirpy)