---
categories:
- AI/ML
- Notes
date: 2024-11-13 00:06:28 +0900
tags:
- langchain
- llm
- openai
- rag
title: '[LangChain] LangChain으로 RAG 구성하기'
uid: 263
---

## RAG(Retrieval Agumented Generation) 이란?

RAG는 생성형 AI 모델이 응답을 생성하기 전에 **사전에 구축된 관련 문서를 검색**하여 보다 정확하고 구체적인 답변을 제공하는 방식이다. 이를 통해 모델은 **학습되지 않은 외부 지식**을 기반으로 하여 최신 정보나 문맥에 맞는 정보를 효과적으로 활용할 수 있다. 

## 핵심 개념

RAG를 구성하기 위해서는 아래의 핵심 5가지로 구성된다. 3가지는 사전에 구축되어야 하는 과정이고, 4~5는 서비스 시에 필요한 개념이다.

1. 문서 로드 
2. 문서 분할
3. 문서 임베딩 및 VectorDB 저장
4. VectorDB 기반 문서 검색
5. Context 기반 쿼리 요청 및 답변 생성

각각의 개념에 대해 코드와 함께 알아본다.

- 1~3번 구조

![image.png](https://i.imgur.com/vwy45G7.png)

- 4~5번 구조

![image.png](https://i.imgur.com/ZDIUJ67.png)

*출처: [https://python.langchain.com/docs/tutorials/rag/#concepts](https://python.langchain.com/docs/tutorials/rag/#concepts)*

## 환경 설정

아래 예제 코드는 vector db로 chroma를 사용할 것이고, langchain의 사용 내역을 확인하기 위해서 langsmith를 사용할 것이다. 

따라서 필요한 패키지를 설치하고 `.env` 파일에 필요한 환경 변수를 지정해준다.

```bash
pip install -qU langchain langchain-community langchain-chroma
```

```python
# .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=<KEY>

OPENAI_API_KEY=<KEY>
```

## 전체 코드 예시

아래는 RAG 구성에 필수적인 코드를 적용한 예시이다. 웹 문서를 불러와 특정 단위로 분리하고 VectorDB에 저장한다. 쿼리 요청시 DB에서 검색 문서를 통해 context와 함께 LLM에 요청한다.

지금 바로 이해하지 못해도 아래에서 하나의 모듈씩 차근차근 살펴보도록 하겠다.

```python
import bs4
from langchain import hub
from langchain_chroma import Chroma
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# Load, chunk and index the contents of the blog.
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())

# Retrieve and generate using the relevant snippets of the blog.
retriever = vectorstore.as_retriever()
prompt = hub.pull("rlm/rag-prompt")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

rag_chain.invoke("What is Task Decomposition? Answer it in Korean.")
```

## 문서 로드

RAG는 LLM 모델이 학습되지 않은 내용이지만 Context로 함께 전달하여 답변할 수 있게 하는 구조이다. 

따라서 사전에 원하는 데이터를 DB로 구축해야 하는데 가장 먼저 필요한 일은 데이터(문서)를 읽어오는 것이다.

LangChain에서는 다양한 문서를 읽을 수 있도록 사전에 필요한 기능들을 구현해 두었다.

아래 코드는 `WebBaseLoader`를 통해 데이터를 가져오지만 이외에도 PDF, CSV, 로컬 폴더 내의 파일들, HTML, JSON, Markdown, Office 등 다양한 포맷의 데이터를 읽어올 수 있다.

> 추가적으로 아래 링크에 들어가면 langchain에서 지원하는 document loader 목록을 확인할 수 있다.
> 
> [https://python.langchain.com/docs/integrations/document_loaders/](https://python.langchain.com/docs/integrations/document_loaders/)
{: .prompt-tip }


```python
import bs4
from langchain_community.document_loaders import WebBaseLoader

# Only keep post title, headers, and content from the full HTML.
bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs={"parse_only": bs4_strainer},
)
docs = loader.load()

len(docs[0].page_content)
# 43131
```

위 코드를 실행시키면 docs에는 `Document` 객체가 page_content와 metadata로 정의되어 저장된다.

metadata에는 문서와 관련된 데이터들(e.g. 소스, 페이지 넘버 등)이 들어있고, page_content에는 텍스트 정보가 들어있다.

![스크린샷 2024-11-12 오후 3.15.43.png](https://i.imgur.com/JRghgtD.png)

코드 상에 bs4_strainer가 들어있는데 이것은 사전에 html 태그의 class 기반으로 원하는 정보만 추출할때 사용한다. [위 링크](https://lilianweng.github.io/posts/2023-06-23-agent/)에 들어가서 확인해보면 아래 이미지처럼 title, content에 해당 되는 div 태그에 post-title, post-content라는 class가 입력되어있는것을 확인할 수 있다.

![스크린샷 2024-11-11 오후 3.03.34.png](https://i.imgur.com/7sR3nfp.png)

## 문서 분할

위에서 load된 문서들을 임베딩 하기 위해서는 작은 단위로 쪼개야 한다. 위의 예시에는 텍스트 길이가 4.3만개이기 때문에 이를 LLM에서 한 번에 처리할 수 없다.  따라서 TextSplitter를 이용하여 텍스트를 Chunk로 만든다.

Splitter 또한 langchain에 구현되어 있는데, 아래에는 `RecursiveCharacterTextSplitter`를 사용한다. 이는 “`\n\n`”, “`\n`”, “ ``“ 등 을 기준으로 나누어 최대한 문단이나 문장, 단어 단위의 관련된 텍스트들을 묶어서 분리한다.

이때 일반적으로 앞, 뒤 문장의 문맥을 파악할 수 있도록 일정 부분 겹치도록 하는데 이를 `chunk_overlap`이라고 한다. 아래에서는 1000개의 size만큼 나누며 200 size 만큼은 겹치도록 설정했다.

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, chunk_overlap=200, add_start_index=True
)
all_splits = text_splitter.split_documents(docs)

len(all_splits)
# 66
```

## 인덱싱

다음 단계로는 위의 66개의 텍스트 청크를 임베딩으로 만들어 인덱싱 한다. 이를 위해 벡터 데이터베이스를 만들고 이곳에 각 임베딩을 삽입한다. 추후에 텍스트 검색을 하면 이 임베딩을 기준으로 “`유사도`”를 측정하여 가장 유사한 임베딩을 추출하고 해당 임베딩의 문서를 반환하게 된다.

> 임베딩: 수의 집합 형태로 [0.1, 0.4, 0.5, …, 0.2] 형태의 벡터이다.
{: .prompt-tip }


아래 코드는 `Chroma`라는 VectorDB를 사용하여 메모리 상에서 DB를 구축한다. (실제 프로덕션 환경에서는 메모리 이외에도 로컬 시스템에 저장하는 DB를 사용해야 한다.)

from_documents라는 함수로 Document 리스트와 Embedding함수를 입력해준다. 

가장 쉽게 사용할 수 있는 임베딩 방법으로는 OpenAI의 Embedding을 사용한다. (`text-embedding-3-large`) 

임베딩은 문서의 양에 따라 비용이 청구될 수 있기 때문에 유의해서 사용한다.

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
```

## 검색 (Retrieval)

그럼 이제 위에서 만든 벡터스토어를 기반으로 텍스트 쿼리를 검색할 수 있게 된다.

아래 코드를 작성하면 vectorstore를 retriever로 쉽게 사용이 가능하고 langchain의 invoke 함수를 사용하면 가장 유사한 documents들이 반환된다.

내부적으로는 쿼리도 임베딩 되어 벡터 스토어 내부의 모든 임베딩 중에 가장 유사한 것들을 추출하는 방식이다.

```python
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 6})
retrieved_docs = retriever.invoke("What are the approaches to Task Decomposition?")
len(retrieved_docs)
# 6
```

아래 retrieved 된 첫번째 문서를 확인해보면 질문에 있던 Task Decomposition과 관련된 문서들을 찾아오는 것을 확인할 수 있다.

```python
print(retrieved_docs[0].page_content)

# Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
# Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
```

## 검색 기반 답변 요청

이제 위에서 가져온 문서들을 Context로 입력 쿼리와 함께 LLM에 요청하면 된다.

일단 Hub에서 RAG용 프롬프트 예제를 가져온다.

```python
from langchain import hub

prompt = hub.pull("rlm/rag-prompt")
```

위에서 rlm/rag-prompt에서 가져온 프롬프트 내용은 아래와 같다.

짧게 요약하면 사용자의 Question과 Retrieval된 Context를 기반으로 답변해주고, Context에 요청 질문에 관한 내용이 없다면 모른다고 답변하라고 하는 것이다.

```python
ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:"), additional_kwargs={})])
```

이제 위의 모든 사항을 chain 으로 엮어서 동작하게끔 수행한다.

- format_docs는 Context로 입력할때 깔끔하게 정돈하기 위해서 문서들 간에 2줄 줄바꿈을 추가해주는 것이다.
- RunnablePassthrough()는 데이터가 다른 로직 없이 그대로 전달되도록 할때 사용한다. 최종으로 구성된 chain에 쿼리를 요청할때 들어가는 값이다.
- 이후에 각각의 key값이 prompt에 적절하게 들어가고 이 전체 텍스트를 LLM에 한번에 요청한다.
- 반환된 값은 AIMessage 객체 형태이지만, StrOutputParser를 사용하여 텍스트만 출력하게 만든다.

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

for chunk in rag_chain.stream("What is Task Decomposition? Answer it in Korean."):
    print(chunk, end="", flush=True)
```

최종 출력은 아래와 같이 한글로 잘 나오게 된다.

```
작업 분해(Task Decomposition)는 복잡한 문제를 여러 단계의 생각으로 나누어 해결하는 방법입니다. 이는 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 형성하는 과정으로 이루어집니다. 이러한 접근 방식은 문제를 더 작고 간단한 작업으로 나누어 처리할 수 있게 합니다.
```

## 더 더 쉽게 RAG 구성하기

위의 과정들을 Langchain에서 아주 고수준의 함수들을 구현해 두었다. 위의 단계들을 1~2줄의 간단한 코드로 쉽게 구현할 수 있도록 하였다. 

`create_retrieval_chain`과 `create_stuff_documents_chain` 두 함수를 이용하면 위의 chain 구조를 쉽게 만들 수 있다. 아래 예시 코드를 보면 쉽게 이해할 수 있다.

llm, prompt, retrieval 세가지 객체만 있으면 바로 rag_chain이 구성된다.

```python
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

response = rag_chain.invoke({"input": "What is Task Decomposition? Answer it in Korean."})
print(response["answer"])
```

같은 질문을 던지면 위와 마찬가지로 같은 비슷한 답변이 한글로 나온다.

```
작업 분해(Task Decomposition)는 복잡한 작업을 더 작고 간단한 단계로 나누는 과정입니다. 체인 오브 사고(Chain of Thought, CoT)와 같은 기법을 사용하여 모델이 "단계별로 생각하라"는 지시를 받아 작업을 관리 가능한 여러 하위 작업으로 변환하도록 합니다. 이를 통해 모델의 사고 과정을 해석하는 데에도 도움이 됩니다.
```

## 결론

이제 최상단의 전체 코드를 다시 보면 이해가 될 것이다. 하지만 이는 아주아주 간단하게 구현된 코드로 실제 프로덕션에서 사용하기 위해서는 추가적인 작업이 필요하다.

- 채팅 형식으로 구현하려면 어떻게 할지
- 메모리 기능은 어떻게 구현할지
- 어떤 Vector DB를 사용할지
- 어떤 Retrieval 알고리즘을 사용할지
- 답변의 결과는 어떻게 평가할지

이외에도 고려할 사항이 많지만 본인의 프로젝트 상황에 맞게 필요한 기능들을 찾아서 추가해 나가면 된다. LangChain 공식 Document 에 활용 사례와 예제 코드가 너무 잘 나와있어서 참고하면 좋을 것 같다.

## 참고자료

[1] [https://python.langchain.com/docs/tutorials/rag/#concepts](https://python.langchain.com/docs/tutorials/rag/#concepts)

Uploaded by [N2C](https://github.com/jmjeon2/Notion2Chirpy)