---
categories:
- AI/ML
- Papers
date: 2024-04-01 20:10:25 +0900
math: true
tags:
- ai
- multimodal
- vlp
title: '[논문 리뷰] BLIP 설명, 리뷰'
uid: 242
---

- Published: 2022.01
- Paper: [https://arxiv.org/pdf/2201.12086.pdf](https://arxiv.org/pdf/2201.12086.pdf)
- Github: [https://github.com/salesforce/BLIP](https://github.com/salesforce/BLIP)

## Introduction

CLIP 논문의 발표 이후로 VLP(Vision-Language Pretraining) 모델을 학습시키기 위해 많은 모델들이 제시되었습니다. 하지만 Large web data의 특성상 image-text pair에서 noise가 많다는 한계점이 있었습니다. (잘못된 caption이 매칭된 경우) BLIP 모델은 이를 개선하고자 새로운 캡션을 생성하고 필터링하는 CapFilt 구조를 만들어서 문제를 해결하고자 했습니다.

또한 기존의 VL 모델들은 구조적 한계 때문에 Image Understanding, Text Generation등에만 좋은 성능이 나왔는데, BLIP은 MED 구조를 제시하여 일반화 성능을 보이는 VLP 방법을 통해 SOTA에 올랐습니다.

> **Contribution**
> 
> 1. Image captioning, Image-Text Retrieval 등의 Task를 모두 잘할 수 있는 새로운 모델 구조 MED 제시
> 2. CapFilt 구조의 모델을 제시하여 web data의 noise caption 문제를 개선
{: .prompt-info }


## Methods

![스크린샷 2024-04-01 오후 4.00.45.png](https://i.imgur.com/d0aYSty.png)

### 1. MED(Multimodal mixture of Encoder-Decoder)

우선 MED는 위 그림과 같이 3가지 모듈로 구성되어 있습니다. 위 그림에서 같은 색으로 되어있는 Block은 weight를 공유하여 학습합니다. 

또한 이미지 모델은 ViT 기반의 모델을 사용하고, 텍스트 모델은 BERT 기반 모델을 사용합니다.

각각의 역할에 대해 하나씩 알아보겠습니다.

### 1. ITC(Image-Text Contrastive Loss)

ITC 모듈은 이미지와 텍스트를 Encoding하여 feature space에서 유사도를 높이기 위해 Contrastive Learning을 진행합니다. 그럼 positive pair한 이미지-텍스트 쌍은 feature space에서 비슷한 값이 나오도록 반대로 negative pair는 다른 값이 출력되도록 학습됩니다.  이때 텍스트 인코더는 [CLS] 토큰을 문장앞에 추가하여 사용합니다.

negative pair에서 noise로 인해 잠재적인 positive 쌍이 있을 수 있으므로 soft label을 사용합니다. 

### 2. ITM(Image-Text Matching)

ITM 모듈은 이미지와 텍스트가 positive/negative 쌍인지 판별하는 이진 분류 모듈입니다. Self Attn과 Feed Forward 레이어 사이에 이미지와 텍스트를 입력으로 받는 Cross Attention Layer를 추가로 학습하여 서로 간의 의미가 학습되도록 합니다. 이때 텍스트 인코더는 [Encode] 토큰을 앞에 추가하여 학습합니다. 이 토큰은 multimodal representation으로 사용됩니다. 

Hard negative mining 방법을 적용하여 contrastive similarity가 높은 negative pair를 선별하여 학습합니다.

### 3. LM(Language Modeling)

LM 모듈은 입력된 이미지로부터 이미지를 설명하는 텍스트를 생성하는 모듈입니다. [Decode] 토큰을 앞에 추가하여 학습합니다.

학습시에 0.1의 smooth-labeling을 적용하여 학습합니다. 

## 2. CapFilt(Captioning and Filtering)

![스크린샷 2024-04-01 오후 5.13.08.png](https://i.imgur.com/r4bGiec.png)

일반적으로 web data는 noise가 많이 포함되어 있기 때문에 한계가 있다고 했습니다. 이러한 문제를 해결하기 위해서 더 좋은 품질의 caption으로 학습할 수 있도록 CapFilt 모델을 제시하였습니다.

우선 web data+Gold GT 로 pre-training을 하고 2차적으로 Bootstrapping을 진행합니다. 여기서 caption을 추가적으로 생성하고 필터링하는 과정이 들어가는데요, 사전에 학습된 텍스트 생성 모델(LM finetune)로 이미지에 맞는 텍스트 $T_s$를 생성합니다. (Captioning)

그리고 raw text $T_w$와 생성된 텍스트 $T_s$를 Filtering 모델(ITC, ITM finetune)로 학습하여 이미지와 pair하다고 분류한 것들만 선별합니다. (Filtering)

이러한 작업들을 통해 web에서 가져온 raw data의 품질을 향상시켜 전반적인 성능을 개선했다고 합니다.

![스크린샷 2024-04-01 오후 5.19.48.png](https://i.imgur.com/au9ayKZ.png)

## Experiments

- Captioning과 Filtering 각각 하나씩 적용할떄에도 성능이 좋아지지만 둘 다 적용할때 가장 높은 성능수치가 나옵니다.
- Backbone ViT 모델의 사이즈가 커질수록 (B→L) 성능이 좋아집니다.

![스크린샷 2024-04-01 오후 5.20.11.png](https://i.imgur.com/2gSd4Xt.png)

- SA를 제외한 모듈의 Weights를 공유하였을때, 학습해야 할 파라미터는 줄어들지만 오히려 성능면에서 좋아지는 실험결과입니다.

![스크린샷 2024-04-01 오후 5.20.53.png](https://i.imgur.com/Y9iXJ52.png)

- 각 모델별 실험결과 BLIP 모델이 COCO, Flicker30K 데이터셋에 대해서 가장 우수한 성능을 보여줍니다.

![스크린샷 2024-04-01 오후 5.22.28.png](https://i.imgur.com/0dXDg5A.png)

## Results

이 모델을 통해서 Image-Text Retrieval, Image Captioning, VQA, NVLR 등에 적용할 수 있다고 합니다. 각 Task마다 약간의 파인튜닝을 통해서 기존 모델 대비 가장 좋은 성능을 내고 있습니다.

해당 모델의 pretrained model은 huggingface에서도 등록되어서 쉽게 사용해볼 수 있고, 각자의 데이터로 파인튜닝할 수 있습니다.

Uploaded by [N2C](https://github.com/jmjeon2/Notion2Chirpy)