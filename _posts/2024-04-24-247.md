---
categories:
- AI/ML
- Papers
date: 2024-04-24 20:09:40 +0900
tags:
- ai
- multimodal
- vlp
title: '[논문 리뷰] BLIP-2 설명, 리뷰'
uid: 247
---

> **[논문리뷰] BLIP-2 논문 및 코드 설명**
> 
- Published: 2023.01
- Paper: [https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597)
- Code: [https://github.com/salesforce/LAVIS/tree/main/projects/blip2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)

> **Summary**
> 
> - BLIP-2는 사전 학습된 이미지 인코더와 대규모 언어 모델을 활용하여 효율적인 Vision-Language 사전 학습 방법을 제안한다.
> - `Q-Former`라는 구조를 도입하여 이미지와 언어 모델 사이의 특징을 효과적으로 연결한다.
> - Zero shot Image-Text Generation, Image Captioning, VQA, Image-Text Retrieval 등의 대부분 Task에서 SOTA 달성
{: .prompt-info }


## Introduction

최근 Vision Language Pretraining(VLP) 연구가 활발히 진행되고 있는데 대부분의 sota 모델들은 end-to-end로 점점 더 큰 규모의 학습과 리소스가 필요하다.

따라서 BLIP2는 pretrained된 Vision Model과 Language Model을 활용하여 전체 모델을 재학습하지 않고 필요한 모듈만 학습하는 형태로 compute-efficient VLP 방법을 제안한다.

여기의 핵심은 cross-modal alignment로 freeze된 두 모델 사이의 feature를 효과적으로 연결해주는 것이 필요하다. 따라서 Querying Transformer (Q-Former) 구조로 두 모델간의 bottleneck 역할을 하며 2-stage로 학습하여 이 문제를 해결한다.

![스크린샷 2024-10-10 오후 4.43.57.png](https://i.imgur.com/uIDJyGC.png)

## Method

위의 방법을 구현하기 위해 2-Stage로 학습한다. 

### 모델 구조

- Q-Former는 Image와 Language의 Feature들 사이간의 gap을 연결해주는 학습가능한 모듈이다.
- Q-Former는 입력 이미지의 해상도와 상관없이 고정된 길이의 feature를 추출한다.
- Q-Former는 아래 이미지와 같이 2가지의 transformer submodule로 구성된다.
    1. Frozen Image Encoder로부터 visual feature를 추출하는 image transformer
    2. 자연어를 인코딩, 디코딩하는 text transformer
- Q-Former는 pretrained BERT로 초기화하여 학습한다.

![스크린샷 2024-10-10 오후 4.48.44.png](https://i.imgur.com/X38SvQf.png)

### Bootstrap Vision-Language Representation Learning (Image Encoder)

![스크린샷 2024-10-10 오후 4.52.57.png](https://i.imgur.com/lCav9eX.png)

**Image-Text Contrastive Learning (ITC)**

- 이미지 feature와 언어 feature를 align 시키도록 학습한다.
- (image-text)의 positive pair는 유사도가 높아지도록, negative pair는 낮아지도록 학습한다.
- 각각의 modality는 각각 feature를 추출하기 때문에 unimodal self-attention mask를 사용한다.

**Image-grounded Text Generation (ITG)**

- 주어진 입력 이미지에 대한 텍스트를 생성하도록 Q-Former를 학습한다.
- 텍스트 생성에 필요한 정보는 입력 이미지에서 먼저 추출한 다음에 자체 모듈을 통해 생성한다.
- 텍스트 생성 시에 이전 토큰에 대해서만 고려하여 feature를 추출해야 하기 때문에 causal self-attention mask를 사용한다.

**Image-Text Matching (ITM)**

- 주어진 image와 text가 paired 된 쌍(positive)인지 아닌지(negative) 분류하는 binary classification를 학습한다.
- 값이 높은 negative pairs를 샘플링하는 hard negative mining 방법을 사용한다.
- 두 pair 데이터에 서로 연관성이 있는지 확인해야 하기 때문에 양방향 Bi-directional self-attention mask를 사용한다.

### **Bootstrap Vision-to-Language Generative Learning (LLM)**

![스크린샷 2024-10-10 오후 5.05.14.png](https://i.imgur.com/bU6aI8V.png)

- 위에서 첫번째로 학습이 완료되면 두번째로는 Q-Former를 LLM 모델과 붙여 학습하는 과정이 필요하다.
- Q-Former의 출력이 LLM 모델에 입력되기 위해서는 dimension을 맞춰주어야 하기 때문에 FC Layer를 붙여준다.
- LLM에는 decoder based model과 encoder-decoder based model 두가지의 경우로 나뉘어서 학습되었다.
- decoder based model은 visual 정보가 포함된 learned query가 frozen LLM 을 통해 텍스트를 잘 생성하도록 학습한다.
- encoder-decoder based model은 visual 정보에 특정 prefix 를 추가하여 인코더에 함께 넣어서 학습한다.

## Pretraining

### Dataset

Pretraining에 사용된 데이터는 BLIP에서 사용한 것과 같고, 총 129M 개의 이미지 데이터를 사용한다.

- COCO
- Visual Genome
- CC3M
- CC12M
- SBU
- LAION400M
- 추가적으로 BLIP에서 제안된 **CapFilt** 기법을 사용하여 이미지 데이터로부터 synthetic caption를 생성하여 noise 여부를 filtering한다.

### Pretraining Image Encoder and LLM

각각의 Frozen 모델은 아래와 같이 사용하였다.

- Image Encoder
    - ViT-L/14 (CLIP)
    - ViT-g/14 (EVA-CLIP
- LLM
    - OPT (decoder based)
    - FlanT5 (encoder-decoder based)

## Experiment

### BLIP2 모델의 결과 예시

![스크린샷 2024-10-14 오후 2.07.03.png](https://i.imgur.com/jAaJ4Ia.png)

### Zero shot Image-to-Text Generation

- 아래와 같이 `zero-shot` 부분에서 타 모델과 대비하여 Params 수는 가장 적지만 각 Task에서 (VQA, Image Captioning, Image-Text Retrieval) 가장 우수한 성능이 나온다.
- zero-shot의 입력 prompt로는 “Question: {} Answer: ” 를 사용했다.

![스크린샷 2024-10-14 오후 2.10.16.png](https://i.imgur.com/x1p1KHQ.png)

- 아래의 실험결과를 통해 가장적은 Train Params에도 불구하고 대부분의 Task에서 우수한 성능을 보인다.
- 실험적으로 전체 구조에 사용된 Image Encoder와 LLM 각각의 성능이 좋을수록 전체 성능이 좋아진다.

![스크린샷 2024-10-14 오후 2.10.24.png](https://i.imgur.com/mlUN3vb.png)

- 아래의 실험결과를 확인해보면 representation learning을 적용했을때와 아닐때의 성능차이게 크게 나는 것을 확인할 수 있다.
- Q-Former가 두 modality의 bridge 역할을 하기 위해서는 **Vision-Language Representation이 필수적**이라고 할 수 있다.

![스크린샷 2024-10-14 오후 2.16.33.png](https://i.imgur.com/ohG73WM.png)

### Image Captioning

- 이미지의 captioning 텍스트를 생성할 때에는 “a photo of” 에 대한 프롬프트를 LLM 입력으로 사용한다.
- 아래 실험 결과를 통해 captioning task에서도 대부분의 task의 성능 지표에서 가장 우수하다.

![스크린샷 2024-10-14 오후 2.16.41.png](https://i.imgur.com/8KXW8WD.png)

### VQA(Visual Question Answering)

- VQA data를 학습할 때에는 Q-Former와 Image Encoder를 fine-tunning한다. (LLM은 frozen)
- Question의 정보는 Q-Former에 입력되어 이미지 feature와 cross-attention 되어 질의와 관련된 이미지의 feature가 더욱 강조되도록 하였다.
- Q-Former와 질의 토큰을 LLM에 함께 입력하여 최종 답변을 출력한다.
- 학습결과는 아래와 같이 VQA에서도 SOTA를 달성하였다.

![스크린샷 2024-10-14 오후 2.30.13.png](https://i.imgur.com/MMAqX0O.png)

### Image-Text Retrieval

- Retrieval은 텍스트 생성이 필요하지 않기 때문에 1-Stage로 Q-Former와 Image Encoder를 fine-tunning한다.
- 학습과 실험 결과는 아래와 같이 BLIP-2 모델이 가장 우수한 성능을 보인다.

![스크린샷 2024-10-14 오후 6.07.29.png](https://i.imgur.com/uoWcNAg.png)

- Retireval Task에서 ITC, ITM은 서로다른 modality feature를 align하는데 필수적이고, ITG 구조(Image-grounded text generation) 또한 Retrieval Task에서의 성능을 더욱 올려준다.

![스크린샷 2024-10-14 오후 6.06.03.png](https://i.imgur.com/STDOQVM.png)

Uploaded by [N2C](https://github.com/jmjeon2/Notion2Chirpy)