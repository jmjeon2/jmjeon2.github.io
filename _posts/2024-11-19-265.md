---
categories:
- AI/ML
- Notes
date: 2024-11-19 23:18:10 +0900
tags:
- langchain
- langgraph
- llm
- openai
title: '[LangChain] 대화형 RAG 구현하기 (LangGraph 사용)'
uid: 265
---

## 개요

지난번에 LangChain으로 RAG를 구축하는 방법을 알아봤다. 하지만 실제 Multi-turn 대화형태가 되기 위해서는 메모리 기능이 필요하다. 기존의 방식으로는 여러번 질문을 해도 처음 묻는것처럼 답변해준다. 이번에는 메모리 기능을 추가하여 실제 챗봇형태의 RAG 구조를 구현해본다.

1. Chain으로 쿼리 요청 시 Retrieval을 수행한다.
2. Agent로 검색을 실행할지에 대한 LLM에게 판단하도록 한다.

## 일반 RAG 코드

우선 필요한 LangChain 관련 함수나 클래스를 선언해준다.

```python
import bs4
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
```

환경 변수와 모델을 정의해준다. (상황에 따라 모델이름을 바뀔 수 있다.)

```python

from dotenv import load_dotenv
load_dotenv()

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model_name='gpt-4o-mini')

```

아래 코드로 웹페이지를 읽어와서 Chunk 단위로 Split 하고 VectorStore에 저장하여 Retriever를 생성한다.

```python
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = InMemoryVectorStore.from_documents(
    documents=splits, embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()
```

RAG기반 QA Chain을 구성하기 위한 시스템 프롬프트를 아래와 같이 작성해준다. 제공되는 context에 근거하여 답변하고 모르면 모른다고 답변하도록 하고, 한글로 답변하도록 한다.

최종적으로 모든 요소를 엮어서 `rag_chain`으로 chain을 구성한다. 

```python
system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise. And Answer it in Korean."

    "\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)
```

그리고 아래와 같이 실행해보면 정상적으로 LLM이 웹페이지를 기반으로 답변해준다.

```python
response = rag_chain.invoke({"input": "Task Decomposition이 뭐야?"})
response["answer"]

# 'Task Decomposition은 복잡한 작업을 더 작고 단순한 단계로 나누는 과정입니다. Chain of Thought(CoT)와 같은 기법을 사용하여 모델이 "단계별로 생각하라"는 지시를 받아 작업을 효율적으로 처리하도록 돕습니다. 이를 통해 큰 작업을 여러 개의 관리 가능한 작업으로 변환하고 모델의 사고 과정을 이해하는 데 도움을 줍니다.'
```

이때 채팅 히스토리를 기억하는 기능은 추가하지 않았기 때문에 아래와 같이 질문하면 LLM은 잘 이해하지 못한다.

```python
response = rag_chain.invoke({"input": "그것을 하기 위해 어떤 것이 필요해?"})
response["answer"]

# '무엇을 하려는지 구체적으로 말씀해 주시면 필요한 것에 대해 더 정확한 정보를 제공할 수 있습니다. 일반적으로 어떤 작업을 수행하기 위해서는 관련 자료, 도구, 기술 및 계획이 필요합니다. 추가적인 세부사항을 알려주시면 더 도움을 드릴 수 있습니다.'
```

따라서 연속적인 Multi-turn 대화가 되기 위해 Chat History를 메모리 형태로 LLM에 같이 넣어주는 작업이 필요하다!

## Conversational RAG

그럼 어떻게 하면 지난 대화기록을 같이 넣어줄 수 있을까?

우선 대화기록이 필요한 부분은 아래 2가지이다. 

1. VectorDB Retrieval 할 때 → 지난 대화를 함께 넣지 않으면 Document 검색이 잘 안 될 수 있음
2. 최종 LLM 응답 요청시 → 지난 대화를 같이 넣어주어야 맥락을 이해하고 답변

이걸 그림으로 나타내면 아래와 같다.

Input Query와 Chat History는 Retrieval을 하기 위해 동시에 입력되고, 또 최종 응답을 요청할때에도 동시에 들어간다.

![스크린샷 2024-11-19 오전 11.16.57.png](https://i.imgur.com/sTFDrQW.png)

### Retrieval 과정

Input query에 대해서만 Retrieval 하면 지난 히스토리에 대한 내용이 없어서 검색이 잘 되지 않을 수 있다. 아래 예시에 반복적으로 사용하겠지만 아래 맥락에서 마지막 메세지 중 ‘그것’ 이라는 단어는 이전 대화의 맥락을 모르면 알 수 없다. 따라서 Retrieval 할 때에는 Chat History를 함께 넣어주어야 한다.

> Human: Task Decomposition이 뭐야?
AI: Task Decomposition은 xxx 입니다.
Human: **그것**을 하기 위해 어떤 것이 필요해?
> 

이때 Embedding은 입력을 벡터로 변환하는데 원래의 Chat History 전체를 단순히 함께 넣어서 만들지 않고, 실제 필요한 쿼리로 재변환한다. 이것이 위 구조에서 `history_aware_retriever`로 표현된다. Contextualize 프롬프트를 LLM에 요청하면 Input Query와 Chat History를 조합하여 Retrieval에 사용하기 적절한 텍스트를 다시 생성해준다. 이 텍스트를 기반으로 Vector DB에서 Document Retrieval 을 하게 된다.

### 구현 코드

우선 `contextualize_q_system_prompt`를 만들어서 Retrieval 에 사용될 입력을 재생성해달라고 요청한다. `ChatPromptTemplate`을 보면 시스템 프롬프트, Chat History, Input query 세가지가 모두 들어가는 것을 볼 수 있다. LangChain에서는 create_history_aware_retriever 함수를 제공하여 쉽게 chain으로 구성할 수 있다.

```python
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder

contextualize_q_system_prompt = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is."
)

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)
```

그럼 이제 실제 QA chain을 구성한다. 위에서 만든 `history_aware_retriever`를 사용하여 `create_retrieval_chain`, `create_stuff_documents_chain` 함수를 이용하여 최종 rag_chain을 구성한다.

```python
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
```

이제 rag_chain으로부터 연속적인 채팅 요청을 해본다. chat_history에 input query, answer를 추가하여 다음 rag_chain 요청시 함께 포함하여 요청한다.

```python
from langchain_core.messages import AIMessage, HumanMessage

chat_history = []

question = "Task Decomposition이 뭐야?"
ai_msg_1 = rag_chain.invoke({"input": question, "chat_history": chat_history})
chat_history.extend(
    [
        HumanMessage(content=question),
        AIMessage(content=ai_msg_1["answer"]),
    ]
)

second_question = "그것을 하기 위해 어떤 것이 필요해?"
ai_msg_2 = rag_chain.invoke({"input": second_question, "chat_history": chat_history})

print(ai_msg_2["answer"])

# Task Decomposition을 수행하기 위해서는 몇 가지 방법이 필요합니다. 첫째, 간단한 프롬프트를 사용하여 LLM에 작업 단계를 요청하거나, 둘째, 작업에 특화된 지침을 제공하거나, 셋째, 인간의 입력을 활용할 수 있습니다. 이러한 방법들을 통해 복잡한 작업을 더 잘 나눌 수 있습니다.
```

위의 생성된 답변을 보면 두번째 질문 “그것을 하기 위해 어떤 것이 필요해?” 의 “그것”을 Task Decomposition이라고 잘 이해하였다. 

### LangSmith 확인

LangSmith를 접속해서 요청에 대한 내역을 들어가면 rag_chain이 내부적으로 어떻게 동작했는지 알 수 있다.

아래와 같은 모듈들을 거쳐서 최종 응답이 생성된다. retrieval_chain에서 ChatOpenAI 요청으로 VectorStore의 Document를 가져오고, 해당 문서들을 기반으로 최종 응답이 생성된다.

![스크린샷 2024-11-19 오후 4.30.04.png](https://i.imgur.com/L75kgFR.png)

retrieve_chain을 좀 더 살펴보면 첫번째 ChatOpenAI 요청시 아래처럼 되는것을 알 수 있다.

시스템 프롬프트, Chat History, 현재의 Input query 세가지가 함께 입력되어 최종으로 
“Task Decomposition을 수행하기 위해 어떤 요소들이 필요한가요?” 라는 문장으로 변환된다.

원래는 “그것을 하기 위해 어떤 것이 필요해?” 라는 문장이였는데 Retrieval 과정에서 변환된 문장이 유사 문서를 찾아오기에 훨씬 적합하도록 변환됐다.

![스크린샷 2024-11-19 오전 10.54.54.png](https://i.imgur.com/oRLiIMz.png)

아래 이미지를 보면 실제 Retriever에는 변환된 질의 “Task Decomposition을 수행하기 위해 어떤 요소들이 필요한가요?” 문장이 입력됐다.

![스크린샷 2024-11-19 오전 10.54.44.png](https://i.imgur.com/Lg0FFBM.png)

> LangChain 0.2 버전 이전에는 RunnableWithMessageHistory를 사용했는데, 0.3 버전부터
> LangGraph의 memory 기능을 쉽게 사용할 수 있어 더이상 RunnableWithMessageHistory를 
> 사용할 필요가 없다.
{: .prompt-tip }


## 채팅 기록 DB 저장

위의 코드는 chat history를 직접 extend하여 추가해줬다. 하지만 LangGraph를 사용하면 이것을 아주 쉽게 구현이 가능하며 별도의 history 저장 로직이 없더라도 multi-turn 앱을 개발할 수 있다.

아래 코드는 메모리 기반이며 실제 프로덕션 환경에서는 모든 메시지들을 db 형태로 저장하여 필요할때 다시 재사용할 수 있도록 SQLite나 Postgres DB로 저장하는 것이 좋다.

### LangGraph Multi-turn 코드 구현

아래의 코드는 LangGraph를 기반으로 만든 Conversational RAG 구현이다. 아래는 주요 키포인트 개념이다.

- Node: 특정 Task를 수행하는 함수 단위. state를 입력으로 받고 state를 출력한다.
- Edge: Node들을 연결하는 경로로 노드 간에 edge로 연결되어 있으면 다음 node를 수행하도록 한다.
- State: 각 노드에서 관리하는 상태. TypedDict로 구현되며 노드의 입출력 값이 된다.

그래프 형태로 구현하면 rag_chain을 수행하는 `call_model` 함수를 하나의 노드로 만들고, memory는 LangGraph에서 사전에 정의된 `MemorySaver()`를 사용한다. 그럼 State에 있는 값이 계속 유지가 되는데 State 클래스 내에 chat_history는 `Annotated[Sequence[BaseMessage], add_messages]`로 구현 되어 있다. 이것은 메시지가 추가되면 덮어쓰지 않고 누적되어 모든 값이 리스트 형태로 유지됨을 알려준다.

```python
from typing import Sequence

from langchain_core.messages import BaseMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict

# 그래프에서 사용될 state를 dict 형태로 정의해준다. rag_chain의 입출력 값이 된다.
class State(TypedDict):
    input: str
    chat_history: Annotated[Sequence[BaseMessage], add_messages]
    context: str
    answer: str

# rag_chain을 활용하여 간단한 python 함수 노드를 만든다.
def call_model(state: State):
    response = rag_chain.invoke(state)
    return {
        "chat_history": [
            HumanMessage(state["input"]),
            AIMessage(response["answer"]),
        ],
        "context": response["context"],
        "answer": response["answer"],
    }

# model 이라는 하나의 노드로 그래프를 구성한다.
workflow = StateGraph(state_schema=State)
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# 그래프를 컴파일하고, state가 유지될 수 있도록 메모리를 추가한다.
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

app 그래프에 쿼리를 입력할때 config를 지정해 줄 수 있다. 이것으로 채팅의 세션을 관리한다. thread_id를 특정 값으로 요청하면 해당 채팅 기록을 메모리에 두고 계속 저장하고 있다.  이제 다시 확인해보기 위해 두 가지 질문을 연속적으로 해본다.

```python
config = {"configurable": {"thread_id": "abc123"}}

result = app.invoke(
    {"input": "Task Decomposition이 뭐야?"},
    config=config,
)
print(result["answer"])

# Task Decomposition은 복잡한 작업을 더 작고 간단한 단계로 나누는 과정입니다. 이를 통해 모델이 문제를 단계별로 생각하고 해결할 수 있도록 도와줍니다. Chain of Thought(CoT) 기법이 일반적으로 사용되며, 사용자가 각각의 하위 목표를 정의하도록 지시합니다.
```

```python
result = app.invoke(
    {"input": "그것을 하기 위해 어떤 것이 필요해?"},
    config=config,
)
print(result["answer"])

# Task Decomposition을 수행하기 위해서는 몇 가지 자원이 필요합니다. 첫째, 정보 검색을 위한 인터넷 접근이 필요하고, 둘째, 장기 기억 관리가 필요하며, 셋째, 간단한 작업을 위임할 수 있는 GPT-3.5 기반의 에이전트가 필요합니다. 또한, 특정 작업에 대한 명확한 지침이나 인간의 입력도 중요합니다.
```

위의 결과를 보면 채팅 히스토리를 갖고 있기 때문에 두번째 질문의 “그것”을 “Task Decomposition”이라고 정확하게 이해하고 답변을 해준다.

### 대화 목록 확인

아래 코드를 사용하면 app에서 사용된 chat 목록을 가져올 수 있다. 

```python
# 대화 목록 확인
chat_history = app.get_state(config).values["chat_history"]
for message in chat_history:
    message.pretty_print()
```

```python
================================[1m Human Message [0m=================================

Task Decomposition이 뭐야?
==================================[1m Ai Message [0m==================================

Task Decomposition은 복잡한 작업을 더 작고 간단한 단계로 나누는 과정입니다. 이를 통해 모델이 문제를 단계별로 생각하고 해결할 수 있도록 도와줍니다. Chain of Thought(CoT) 기법이 일반적으로 사용되며, 사용자가 각각의 하위 목표를 정의하도록 지시합니다.
================================[1m Human Message [0m=================================

그것을 하기 위해 어떤 것이 필요해?
==================================[1m Ai Message [0m==================================

Task Decomposition을 수행하기 위해서는 몇 가지 자원이 필요합니다. 첫째, 정보 검색을 위한 인터넷 접근이 필요하고, 둘째, 장기 기억 관리가 필요하며, 셋째, 간단한 작업을 위임할 수 있는 GPT-3.5 기반의 에이전트가 필요합니다. 또한, 특정 작업에 대한 명확한 지침이나 인간의 입력도 중요합니다.

```

## 결론

대화형 RAG를 구현하기 위해서는 지난 채팅 기록에 대한 메모리 기능이 필요하고, 이를 구현하기 위해 LangChain과 LangGraph 코드로 살펴봤다. 좀 더 간결한 코드와 쉽게 구현하기 위해서는 LangGraph를 사용하는 편이 더 쉬울 것 같다. 굉장히 High-level로 함수나 클래스들이 구현되어 있어서 사용하기는 어렵지 않고, 어떻게 chain이 구성되는지 컨셉을 잘 이해하면 좋을 것 같다. 

## 참고자료

[1] [https://python.langchain.com/docs/tutorials/qa_chat_history/](https://python.langchain.com/docs/tutorials/qa_chat_history/)

Uploaded by [N2C](https://github.com/jmjeon2/Notion2Chirpy)