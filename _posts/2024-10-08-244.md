---
categories:
- AI/ML
- Notes
date: 2024-10-08 00:20:48 +0900
math: true
tags:
- faiss
- vector
- vectordb
title: '[VectorDB] FAISS 사용 방법, 검색 원리'
uid: 244
---

## FAISS 개요

Faiss는 Vector DB 를 클러스터링하고 효율적으로 유사성을 검색해주는 라이브러리이다. Faiss는 python 언어로 사용하지만 C++ 기반으로 작성되어 속도가 매우 빠르다. 또한 일부 알고리즘은 GPU를 지원하여 더욱 빠른 속도로 지원이 가능하다.

## FAISS 사용하기

### FAISS 설치

아래의 명령어를 실행하여 cpu 버전 혹은 gpu 버전을 받을 수 있다.

```bash
pip install faiss-cpu
```

```bash
pip install faiss-gpu
```

### FAISS 사용법

아래와 같이 각 벡터를 초기화해준다. faiss에서 사용하는 벡터의 기본 자료형은 `float32`이다.

- 파라미터 설명
    - d: 벡터의 dimension
    - nb: 벡터 데이터베이스의 크기(개수)
    - nq: 쿼리할 벡터의 크기

```python
import numpy as np
d = 64                           # dimension
nb = 100000                      # database size
nq = 10000                       # nb of queries
np.random.seed(1234)             # make reproducible
xb = np.random.random((nb, d)).astype('float32')
xb[:, 0] += np.arange(nb) / 1000.
xq = np.random.random((nq, d)).astype('float32')
xq[:, 0] += np.arange(nq) / 1000.
```

**IndexFlatL2**

IndexFlatL2를 사용하면 Brute-Force로 모든 벡터들을 검색한다.

> L2 거리: 두 벡터 사이의 기하학적 거리이며 수식으로는 다음과 같다. (유클리디언 거리)
{: .prompt-tip }


$$
L2 = ||\textbf{p}-\textbf{q}|| = \sqrt{(p_1-p_2)^2+ ... + (p_n-q_n)^2}
$$

> Brute Force: 모든 가능한 경우의 수를 계산함. Vector DB의 경우 쿼리벡터와 DB에 있는 모든 벡터를 연산하여 비교하므로 시간 복잡도가 O(n)이 됨.
{: .prompt-tip }


- 파라미터 설명
    - k: 유사도가 높은 순으로 찾을 nearest neighbor 벡터의 개수
- 다른 알고리즘과 달리 DB에 있는 모든 벡터와 비교 연산을 하기 때문에 별도의 train이 필요 없으므로 `is_trained` 값이 항상 `True`이다.
- search 함수를 통해 쿼리 벡터와 가장 유사한 벡터를 k개수만큼 반환한다.
- **D**는 distance를 의미하고 **I**는 Index를 의미한다.

```python
import faiss                   
index = faiss.IndexFlatL2(d)   # build the index FlatL2는 L2 거리를 사용하는 인덱스를 생성한다
print(index.is_trained)        # True
index.add(xb)                  # add vectors to the index
print(index.ntotal)            # 100000

k = 4                          # we want to see 4 nearest neighbors
D, I = index.search(xb[:5], k) # sanity check (xb의 첫 5개의 데이터에 대해 가장 가까운 4개의 이웃 벡터를 찾는다)
print(I) # index 출력 (각 row의 첫번째 값으로 0, 1, 2, 3, 4가 각각 나와야 한다)
print(D) # 거리 출력 (각 row의 첫번째 값으로 0이 나와야 한다 (자기 자신과의 거리))
```

```python
# output
[[  0 393 363  78]
 [  1 555 277 364]
 [  2 304 101  13]
 [  3 173  18 182]
 [  4 288 370 531]]
[[0.        7.1751733 7.2076297 7.251163 ]
 [0.        6.3235645 6.684581  6.7999454]
 [0.        5.7964087 6.3917365 7.2815123]
 [0.        7.2779055 7.527987  7.6628466]
 [0.        6.7638035 7.2951207 7.368815 ]]
```

위와 같이 D 출력을 보면 각 행의 첫번째 index는 자기자신이 나오고, I 출력을 보면 distance가 0임을 확인할 수 있다.

**IndexIVFFlat**

위의 IndexFlatL2 방법은 DB 내의 모든 벡터와 연산을 하기 때문에 DB에 저장된 벡터의 개수가 증가 할수록 속도가 매우 떨어지게 된다. 이를 보완하기 위해 IndexIVFFlat을 사용하면 좀 더 빠른 속도로 검색할 수 있다.

> IVF: Inverted File Index의 약자로, 벡터를 여러 클러스터로 나눈 후 각 클러스터 안에서만 검색하는 방식이다(kmeans 알고리즘). 쿼리 벡터가 어느 클러스터에 속하는지 찾고(각 클러스터의 중심벡터와 비교) 해당 클러스터 내에서만 검색하여 검색 속도를 개선시킨다.
{: .prompt-tip }


- 파라미터 설명
    - nlist: 분할 클러스터의 개수
    - nprobe: 각 쿼리에 대해 몇 개의 클러스터를 검색할지 결정 (default: 1)
- quantizer는 `IndexFlatL2`를 사용하여 벡터끼리 거리를 계산할때 사용한다.
- 이전과 달리 knn 알고리즘으로 train을 해주어야 사용 가능하므로 index를 선언후에 `train()` 함수를 호출해야 사용이 가능하다.

```python
nlist = 100 # number of clusters
k = 4
quantizer = faiss.IndexFlatL2(d)  # the other index
index = faiss.IndexIVFFlat(quantizer, d, nlist)
assert not index.is_trained
index.train(xb)
assert index.is_trained

index.add(xb)                  # add dataset
D, I = index.search(xq, k)     # actual search
print(I[-5:])                  # neighbors of the 5 last queries
index.nprobe = 10              # nprobe는 각 쿼리에 대해 몇 개의 클러스터를 검색할지 결정
D, I = index.search(xq, k)
print(I[-5:])                  # neighbors of the 5 last queries
```

```python
# output
[[ 9900  9309  9810 10048]
 [11055 10895 10812 11321]
 [11353 10164  9787 10719]
 [10571 10664 10632 10203]
 [ 9628  9554  9582 10304]]
[[ 9900 10500  9309  9831]
 [11055 10895 10812 11321]
 [11353 11103 10164  9787]
 [10571 10664 10632  9638]
 [ 9628  9554 10036  9582]]
```

위의 결과를 보면 nprobe를 1에서 10으로 바꿔줬을때 검색된 벡터들의 인덱싱이 조금씩 달라진 걸 확인할 수 있다. nprobe 값을 통해 속도와 정확도 간의 균형을 조절할 수 있다. (nprobe가 클수록 정확도는 올라가지만, 속도는 낮아짐)

**IndexIVFQP**

위의 IndexIVFFlat는 벡터 전체를 저장한다. 이때 벡터의 크기가 커지고, 개수도 많아지면 메모리면에서 굉장히 불리해질 수 있다. 따라서 IndexIVFPQ를 사용하여 메모리를 효율적이게 저장하고 사용할 수 있다.

메모리에 이점이 있지만 그에따라 검색 성능이 떨어질 수 있어 프로젝트의 상황에 따라 적절한 파라미터와 알고리즘을 사용해야 한다.

> PQ: Product Quantizer의 약자로, 원래의 벡터를 하위 벡터로 나누고, 각 하위 벡터를 양자화하여 손실압축으로 저장된 벡터를 사용한다.
{: .prompt-tip }


> 양자화: 각 서브벡터를 또다시 kmeans 알고리즘을 통해 256 개의 클러스터(8bit) 로 분할하게 되고 이때 각 서브벡터의 가장 가까운 클러스터 중심점으로 근사화된 값을 사용한다. 이때 중심점의 벡터값을 저장하지 않고 해당 클러스터의 index만을 저장하여 메모리를 효율적으로 사용할 수 있게 된다.
{: .prompt-tip }


- 예시 이미지

![스크린샷 2024-10-08 오후 5.25.56.png](https://i.imgur.com/e5eTF3Y.png)

- 파라미터 설명
    - m: subquantizer의 개수 (d는 m의 배수여야 함)
        
        e.g. d=64, m=8인 경우 이 벡터는 8개의 서브벡터로 나뉘고 각 서브벡터는 8차원(=64/8)이 된다. 이후에 서브벡터는 독립적으로 양자화된다.
        
    - IndexIVFPQ의 마지막 파라미터 8의 의미
        
        위의 각 서브벡터가 8bit(1byte)로 인코딩 되는 것을 의미함.
        e.g. d=64, m=8인경우 서브벡터는 8차원인데, 각 서브벡터를 8bit(1byte)로 압축함.
        한 요소의 값이 32bit(=4byte)인 8차원의 서브벡터를 1byte로 만드므로 압축률은 8*4/1 = 32이다.
        

```python
nlist = 100
m = 8                             # number of subquantizers
k = 4
quantizer = faiss.IndexFlatL2(d)  
index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)
                                    # 8 specifies that each sub-vector is encoded as 8 bits
index.train(xb)
index.add(xb)
D, I = index.search(xb[:5], k) # sanity check
print(I)
print(D)
index.nprobe = 10              # make comparable with experiment above
D, I = index.search(xq, k)     # search
print(I[-5:])
```

```python
# output
[[   0   78  424  753]
 [   1 1063  555  617]
 [   2  304  134  179]
 [   3   64  527 1057]
 [   4  288  531  827]]
[[1.5518408 6.273082  6.416301  6.5346537]
 [1.4250197 5.6698246 6.129957  6.5875382]
 [1.7222786 5.677806  6.116646  6.123891 ]
 [1.8276348 6.7033954 6.9841585 7.0637865]
 [1.5124168 5.6538734 6.3072824 6.457743 ]]
[[ 9853  9966 10914 10437]
 [10765 10403  9014 10240]
 [11291 10600 11383 10494]
 [10005 10664 10122 10125]
 [ 9905  9229 10304 10370]]

```

위의 첫번째 I 출력을 보면 자기자신에 대한 벡터가 나온다. 하지만 D 출력을 보면 위와 다르게 값이 0이 아닌데 이는 양자화되어 저장되었기 때문에 거리가 0이 아닌 근사치가 나오게 된다.

> DB에 있는 동일한 쿼리를 입력하는 경우에 실제 쿼리는 raw 벡터로, db에 저장된 벡터들은 양자화된 값으로 연산하기 때문에 0이 나올 수 없다.
{: .prompt-tip }


### 검색 내부 동작 원리

위 코드에서는 `search()` 함수를 실행하면 알아서 가장 유사성이 큰 벡터들을 가져오는데 실제 내부적으로 어떤 흐름을 통해 가져오는지 알아본다.

1. 쿼리 벡터 q가 입력되면 IVF 인덱스를 통해 각 클러스터의 중심점들 중 가장 가까운 클러스터를 찾는다.
2. 1번에서 찾은 클러스터 내에 있는 모든 벡터와 쿼리 벡터 q를 비교한다.
3. 이때 DB에 있는 벡터는 서브벡터의 양자화된 인덱스 값을 가지고 있다. 따라서 이를 원래 크기의 전체 벡터로 복원하여(근사값) 쿼리벡터 q와 복원된 벡터들과의 유사도를 계산한다.

> **양자화된 인덱스를 전체 벡터로 복원한다는 건?**
> 
> PQ를 통해 각각의 서브벡터들은 256개의 클러스터(8bit) 중 가장 가까운 하나의 클러스터 인덱스 값만을 가지고 있다. 따라서 이 클러스터의 중심점 값들을 사용하여 각 인덱스를 서브벡터로 변환하고 각 서브벡터를 합쳐 전체 크기의 벡터가 된다.
{: .prompt-tip }


![스크린샷 2024-10-08 오후 5.24.56.png](https://i.imgur.com/vDW3BvL.png)

## 참고자료

[1] [https://github.com/facebookresearch/faiss/wiki](https://github.com/facebookresearch/faiss/wiki)

[2] [https://pangyoalto.com/faiss-1-hnsw/](https://pangyoalto.com/faiss-1-hnsw/)

Uploaded by [N2C](https://github.com/jmjeon2/Notion2Chirpy)