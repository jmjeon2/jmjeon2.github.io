---
categories:
- AI/ML
- Notes
date: 2024-10-08 22:25:03 +0900
math: true
tags:
- faiss
- vector
- vectordb
title: '[VectorDB] FAISS 사용 방법, 검색 원리'
uid: 244
---

## FAISS 개요

Faiss는 Vector DB 를 클러스터링하고 유사도를 기반으로 효율적으로 검색해주는 라이브러리이다. Faiss는 python 언어로 사용하지만 C++ 기반으로 작성되어 속도가 매우 빠르다. 또한 일부 알고리즘은 GPU를 지원하여 더욱 빠른 속도로 지원이 가능하다.

## FAISS 사용하기

### FAISS 설치

아래의 명령어를 실행하여 cpu 버전 혹은 gpu 버전을 받을 수 있다.

```bash
pip install faiss-cpu
```

```bash
pip install faiss-gpu
```

### FAISS 사용법

아래와 같이 각 벡터를 초기화해준다. faiss에서 사용하는 벡터의 기본 자료형은 `float32`이다.

- 파라미터 설명
    - d: 벡터의 dimension
    - nb: 벡터 데이터베이스의 크기(개수)
    - nq: 쿼리할 벡터의 크기

```python
import numpy as np
d = 64                           # dimension
nb = 100000                      # database size
nq = 10000                       # nb of queries
np.random.seed(1234)             # make reproducible
xb = np.random.random((nb, d)).astype('float32')
xb[:, 0] += np.arange(nb) / 1000.
xq = np.random.random((nq, d)).astype('float32')
xq[:, 0] += np.arange(nq) / 1000.
```

**IndexFlatL2**

IndexFlatL2를 사용하면 Brute-Force로 모든 벡터들을 검색한다. 

> L2 거리: 두 벡터 사이의 기하학적 거리이며 수식으로는 다음과 같다. (유클리디언 거리)
{: .prompt-tip }


$$
L2 = ||\textbf{p}-\textbf{q}|| = \sqrt{(p_1-p_2)^2+ ... + (p_n-q_n)^2}
$$

> Brute Force: 모든 가능한 경우의 수를 계산함. Vector DB의 경우 쿼리벡터와 DB에 있는 모든 벡터를 연산하여 비교하므로 시간 복잡도가 O(n)이 됨.
{: .prompt-tip }


- 파라미터 설명
    - k: 유사도가 높은 순으로 찾을 nearest neighbor 벡터의 개수
- 다른 알고리즘과 달리 DB에 있는 모든 벡터와 비교 연산을 하기 때문에 별도의 train이 필요 없으므로 `is_trained` 값이 항상 `True`이다.
- search 함수를 통해 쿼리 벡터와 가장 유사한 벡터를 k개수만큼 반환한다.
- **D**는 distance를 의미하고 **I**는 Index를 의미한다.

```python
import faiss                   
index = faiss.IndexFlatL2(d)   # build the index FlatL2는 L2 거리를 사용하는 인덱스를 생성한다
print(index.is_trained)        # True
index.add(xb)                  # add vectors to the index
print(index.ntotal)            # 100000

k = 4                          # we want to see 4 nearest neighbors
D, I = index.search(xb[:5], k) # sanity check (xb의 첫 5개의 데이터에 대해 가장 가까운 4개의 이웃 벡터를 찾는다)
print(I) # index 출력 (각 row의 첫번째 값으로 0, 1, 2, 3, 4가 각각 나와야 한다)
print(D) # 거리 출력 (각 row의 첫번째 값으로 0이 나와야 한다 (자기 자신과의 거리))
```

```python
# output
[[  0 393 363  78]
 [  1 555 277 364]
 [  2 304 101  13]
 [  3 173  18 182]
 [  4 288 370 531]]
[[0.        7.1751733 7.2076297 7.251163 ]
 [0.        6.3235645 6.684581  6.7999454]
 [0.        5.7964087 6.3917365 7.2815123]
 [0.        7.2779055 7.527987  7.6628466]
 [0.        6.7638035 7.2951207 7.368815 ]]
```

위와 같이 D 출력을 보면 각 행의 첫번째 index는 자기자신이 나오고, I 출력을 보면 distance가 0임을 확인할 수 있다.

**IndexIVFFlat**

위의 IndexFlatL2 방법은 DB 내의 모든 벡터와 연산을 하기 때문에 DB에 저장된 벡터의 개수가 증가 할수록 속도가 매우 떨어지게 된다. 이를 보완하기 위해 IndexIVFFlat을 사용하면 좀 더 빠른 속도로 검색할 수 있다.

> IVF: Inverted File Index의 약자로, 벡터를 k-means 알고리즘으로 여러 클러스터로 나눈 뒤에, 가장 가까운 클러스터 안에서만 검색하는 방식이다. 입력 쿼리 벡터를 각 클러스터의 중심점들과 비교하여 어느 클러스터에 속하는지 찾고 해당 클러스터 내에서만 검색하여 검색 속도를 개선시킨다.
{: .prompt-tip }


- 파라미터 설명
    - nlist: 분할 클러스터의 개수
    - nprobe: 각 쿼리에 대해 몇 개의 클러스터를 검색할지 결정 (default: 1)
- quantizer는 `IndexFlatL2`를 사용하여 벡터끼리 거리를 계산할때 사용한다.
- 이전과 달리 k-means 알고리즘으로 train을 해주어야 사용 가능하므로 index를 선언후에 `train()` 함수를 호출해야 사용이 가능하다.

```python
nlist = 100 # number of clusters
k = 4
quantizer = faiss.IndexFlatL2(d)  # the other index
index = faiss.IndexIVFFlat(quantizer, d, nlist)
assert not index.is_trained
index.train(xb)
assert index.is_trained

index.add(xb)                  # add dataset
D, I = index.search(xq, k)     # actual search
print(I[-5:])                  # neighbors of the 5 last queries
index.nprobe = 10              # nprobe는 각 쿼리에 대해 몇 개의 클러스터를 검색할지 결정
D, I = index.search(xq, k)
print(I[-5:])                  # neighbors of the 5 last queries
```

```python
# output
[[ 9900  9309  9810 10048]
 [11055 10895 10812 11321]
 [11353 10164  9787 10719]
 [10571 10664 10632 10203]
 [ 9628  9554  9582 10304]]
[[ 9900 10500  9309  9831]
 [11055 10895 10812 11321]
 [11353 11103 10164  9787]
 [10571 10664 10632  9638]
 [ 9628  9554 10036  9582]]
```

위의 결과를 보면 nprobe를 1에서 10으로 바꿔줬을때 검색된 벡터들의 인덱싱이 조금씩 달라진 걸 확인할 수 있다. nprobe 값을 통해 속도와 정확도 간의 균형을 조절할 수 있다. (nprobe가 클수록 정확도는 올라가지만, 속도는 낮아짐)

> 예를 들어, 입력 쿼리가 두 클러스터 사이 경계 근처에 있을때 하나의 클러스터만 검색하는 경우 해당 클러스터 안의 벡터들과 가장 유사하다는 것을 보장할 수 없다. 따라서 nprobe를 증가하여 정확도를 높일 수 있다.
{: .prompt-info }


**IndexIVFQP**

위의 IndexIVFFlat는 벡터 전체를 저장한다. 이때 벡터의 크기가 커지고, 개수도 많아지면 메모리면에서 굉장히 불리해질 수 있다. 따라서 **IndexIVFPQ를 사용하여 메모리를 효율적이게 저장**하고 사용할 수 있다.

메모리에 이점이 있지만 그에따라 검색 성능이 떨어질 수 있어 프로젝트의 상황에 따라 적절한 파라미터와 알고리즘을 사용해야 한다.

> PQ: Product Quantizer의 약자로, 원래의 벡터를 하위 벡터로 나누고, 각 하위 벡터를 양자화하여 손실압축으로 저장된 벡터를 사용한다.
{: .prompt-tip }


> 양자화: 각 서브벡터를 또다시 kmeans 알고리즘을 통해 256 개의 클러스터(8bit) 로 분할하게 되고 이때 각 서브벡터의 가장 가까운 클러스터 중심점으로 근사화된 값을 사용한다. 이때 중심점의 벡터값을 저장하지 않고 해당 클러스터의 index만을 저장하여 메모리를 효율적으로 사용할 수 있게 된다.
{: .prompt-tip }


메모리 효율성

- 아래와 같은 파라미터로 셋팅되는 경우 64*32bit의 데이터가 4*8bit가 되므로 압축률이 64정도가 된다.
- 실제 Production 환경에서는 m과 code_size를 조정하여 훨씬 큰 압축률로 메모리 효율을 높일 수 있다.

![스크린샷 2024-10-08 오후 5.25.56.png](https://i.imgur.com/yY92U0B.png)

**PQ 인코딩 저장 방법**

1. 원래 크기의 벡터를 서브벡터로 나눈다.
2. 각 서브벡터는 k-means 알고리즘으로 학습하여 각 clustering을 구성한다.
3. 각 서브벡터가 소속되는 cluster의 index값만을 저장한다.

![스크린샷 2024-10-15 오후 5.11.57.png](https://i.imgur.com/s25e5my.png)

- 파라미터 설명
    - m: subquantizer의 개수 (d는 m의 배수여야 함)
        
        e.g. d=64, m=8인 경우 이 벡터는 8개의 서브벡터로 나뉘고 각 서브벡터는 8차원(=64/8)이 된다. 이후에 서브벡터는 독립적으로 양자화된다.
        
    - IndexIVFPQ의 마지막 파라미터 8의 의미
        
        위의 각 서브벡터가 8bit(1byte)로 인코딩 되는 것을 의미함 (code_size)
        e.g. d=64, m=8인경우 서브벡터는 8차원인데, 각 서브벡터를 8bit(1byte)로 압축함.
        한 요소의 값이 32bit(=4byte)인 8차원의 서브벡터를 1byte로 만드므로 압축률은 8*4/1 = 32이다.
        

```python
nlist = 100
m = 8                             # number of subquantizers
k = 4
quantizer = faiss.IndexFlatL2(d)  
index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)
                                    # 8 specifies that each sub-vector is encoded as 8 bits
index.train(xb)
index.add(xb)
D, I = index.search(xb[:5], k) # sanity check
print(I)
print(D)
index.nprobe = 10              # make comparable with experiment above
D, I = index.search(xq, k)     # search
print(I[-5:])
```

```python
# output
[[   0   78  424  753]
 [   1 1063  555  617]
 [   2  304  134  179]
 [   3   64  527 1057]
 [   4  288  531  827]]
[[1.5518408 6.273082  6.416301  6.5346537]
 [1.4250197 5.6698246 6.129957  6.5875382]
 [1.7222786 5.677806  6.116646  6.123891 ]
 [1.8276348 6.7033954 6.9841585 7.0637865]
 [1.5124168 5.6538734 6.3072824 6.457743 ]]
[[ 9853  9966 10914 10437]
 [10765 10403  9014 10240]
 [11291 10600 11383 10494]
 [10005 10664 10122 10125]
 [ 9905  9229 10304 10370]]

```

위의 첫번째 I 출력을 보면 자기자신에 대한 벡터가 나온다. 하지만 D 출력을 보면 위와 다르게 값이 0이 아닌데 이는 양자화되어 저장되었기 때문에 거리가 0이 아닌 근사치가 나오게 된다.

> DB에 있는 동일한 쿼리를 입력하는 경우에 실제 쿼리는 raw 벡터로, db에 저장된 벡터들은 양자화된 값으로 연산하기 때문에 0이 나올 수 없다.
{: .prompt-tip }


### 검색 내부 동작 원리

위 코드에서는 `search()` 함수를 실행하면 알아서 가장 유사성이 큰 벡터들을 가져오는데 실제 내부적으로 어떤 흐름을 통해 가져오는지 알아본다.

1. 쿼리 벡터 q가 입력되면 IVF 인덱스를 통해 각 클러스터의 중심점들 중 가장 가까운 클러스터를 찾는다.
2. 1번에서 찾은 클러스터 내에 있는 모든 벡터와 쿼리 벡터 q를 비교한다.
3. 이때 DB에 있는 벡터는 서브벡터의 양자화된 인덱스 값을 가지고 있다. 따라서 이를 각 인덱스에 해당되는 클러스터와 쿼리의 서브벡터의 distance(근사값)를 누적 계산한다.

> **서브벡터의 근사값들을 누적 계산하는 방법**
> 
> DB 내에 있는 데이터는 PQ를 통해 실제 벡터가 아닌 256개의 클러스터(8bit) 중 가장 가까운 클러스터의 인덱스 값이다. 따라서 이 인덱스 값으로부터 해당 클러스터의 중심점(centroid)의 벡터를 대체하여 사용한다. 즉, 쿼리벡터의 각 서브벡터와 PQ code에 해당되는 클러스터의 centroid(중심점)의 distance(근사값)들을 누적해서 합하여 계산한다.
{: .prompt-tip }


![스크린샷 2024-10-15 오후 5.11.21.png](https://i.imgur.com/E9tH401.png)

실제로는 DB 내에 모든 임베딩에 대해 각각 index→vector 변환 하지 않고, 같은 cluster의 index는 같은 centroid를 공유하기 때문에 아래와 같이 **d** 벡터를 만들어 두고 해당되는 값들만 가져와 계산하여 훨씬 빠르게 검색을 수행한다.

![스크린샷 2024-10-15 오후 5.35.51.png](https://i.imgur.com/C8JpkvO.png)

## 참고자료

[1] [https://github.com/facebookresearch/faiss/wiki](https://github.com/facebookresearch/faiss/wiki)

[2] [https://pangyoalto.com/faiss-1-hnsw/](https://pangyoalto.com/faiss-1-hnsw/)

[3] [https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701](https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701)

Uploaded by [N2C](https://github.com/jmjeon2/Notion2Chirpy)